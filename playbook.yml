---
- name: Observability Stack for EKS - Scenario 1 (Auto Scaling Pod + Node)
  hosts: localhost
  connection: local
  gather_facts: false

  vars:
    kube_context: "{{ kube_context | default('eks-observability') }}"
    ns_monitoring: "{{ monitoring_ns | default('monitoring') }}"
    app_ns: "{{ app_ns | default('petclinic') }}"

  tasks:
    # -------------------- 1Ô∏è‚É£ Namespaces & Helm Repositories --------------------
    - name: Ensure monitoring namespace exists
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ ns_monitoring }}"
        state: present
      tags: ['bootstrap']

    - name: Add / Update Helm repositories
      kubernetes.core.helm_repository:
        name: "{{ item.name }}"
        repo_url: "{{ item.url }}"
      loop:
        - { name: prometheus-community, url: "https://prometheus-community.github.io/helm-charts" }
        - { name: grafana, url: "https://grafana.github.io/helm-charts" }
        - { name: metrics-server, url: "https://kubernetes-sigs.github.io/metrics-server/" }
        - { name: autoscaler, url: "https://kubernetes.github.io/autoscaler" }
      tags: ['bootstrap']

    - name: Update Helm repositories
      ansible.builtin.command: helm repo update
      register: helm_update
      changed_when: "'Update Complete' in helm_update.stdout"
      tags: ['bootstrap']

    # -------------------- 2Ô∏è‚É£ Metrics Server --------------------
    - name: Install metrics-server
      kubernetes.core.helm:
        name: metrics-server
        chart_ref: metrics-server/metrics-server
        release_namespace: kube-system
        wait: true
        timeout: 600s
        values:
          args:
            - --kubelet-insecure-tls
            - --kubelet-preferred-address-types=InternalIP
            - --metric-resolution=15s
      tags: ['metrics']

    # -------------------- 3Ô∏è‚É£ Prometheus + Grafana --------------------
    - name: Install kube-prometheus-stack
      kubernetes.core.helm:
        name: kps
        chart_ref: prometheus-community/kube-prometheus-stack
        release_namespace: "{{ ns_monitoring }}"
        values: "{{ lookup('file', 'files/values-kps.yaml') | from_yaml }}"
        wait: true
        timeout: 900s
      tags: ['kps']

    # -------------------- 4Ô∏è‚É£ Loki + Promtail --------------------
    # üß± Namespace check (ph√≤ng khi ch·∫°y ri√™ng tag 'loki')
    - name: Ensure monitoring namespace exists
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ ns_monitoring }}"
        state: present
      tags: ['loki']

    # ü™£ Loki (Log Aggregation)
    - name: Install Loki
      kubernetes.core.helm:
        name: loki
        chart_ref: grafana/loki
        release_namespace: "{{ ns_monitoring }}"
        values_files:
          - files/values-loki.yaml
        wait: true
        timeout: 900s
      tags: ['loki']

    # üß© Promtail (Log Collector Agent)
    - name: Install Promtail
      kubernetes.core.helm:
        name: promtail
        chart_ref: grafana/promtail
        release_namespace: "{{ ns_monitoring }}"
        values_files:
          - files/values-promtail.yaml
        wait: true
        timeout: 600s
      tags: ['promtail']

    # üîç Apply ServiceMonitor for Loki (so Prometheus can scrape Loki metrics)
    - name: Apply ServiceMonitor for Loki
      kubernetes.core.k8s:
        state: present
        src: manifests/servicemonitor-loki.yaml
      tags: ['loki']

    # ‚è± Pause briefly to allow Prometheus to discover Loki target
    - name: Wait 10s for Prometheus to register Loki targets
      ansible.builtin.pause:
        seconds: 10
      tags: ['loki']

    # ‚ö° Reload Grafana datasource config (ensure Loki visible)
    - name: Re-apply Grafana datasources (including Loki)
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('file', 'files/grafana/provisioning/datasources-configmap.yaml') | from_yaml }}"
      tags: ['grafana', 'loki']

    - name: Restart Grafana Deployment to reload datasources
      kubernetes.core.k8s:
        state: patched
        kind: Deployment
        name: kps-grafana
        namespace: "{{ ns_monitoring }}"
        definition:
          spec:
            template:
              metadata:
                annotations:
                  redeploy-timestamp: "{{ lookup('pipe', 'date +%s') }}"
      tags: ['grafana', 'loki']

    # # üß≠ Tempo (Distributed Tracing)
    # - name: Install Tempo
    #   kubernetes.core.helm:
    #     name: tempo
    #     chart_ref: grafana/tempo
    #     release_namespace: "{{ ns_monitoring }}"
    #     values_files:
    #       - files/values-tempo.yaml
    #     wait: true
    #     timeout: 900s
    #   tags: ['tempo']

    # # -------------------- 5Ô∏è‚É£ Icinga2 (Deployment + Service + Web admin) --------------------
    # - name: Deploy Icinga2 Deployment
    #   kubernetes.core.k8s:
    #     state: present
    #     src: "manifests/icinga-deployment.yaml"
    #   tags: ['icinga']

    # - name: Deploy Icinga2 Service (Core API)
    #   kubernetes.core.k8s:
    #     state: present
    #     src: "manifests/icinga-service.yaml"
    #   tags: ['icinga']

    # - name: Deploy Icinga Web 2 Service
    #   kubernetes.core.k8s:
    #     state: present
    #     src: "manifests/icingaweb2-service.yaml"
    #   tags: ['icinga']

    # - name: Get Icinga2 pod
    #   kubernetes.core.k8s_info:
    #     api_version: v1
    #     kind: Pod
    #     namespace: "{{ ns_monitoring }}"
    #     label_selectors:
    #       - app=icinga2
    #   register: icinga_pods
    #   tags: ['icinga']

    # - name: Set icinga_pod fact
    #   set_fact:
    #     icinga_pod: "{{ icinga_pods.resources[0].metadata.name }}"
    #   when: icinga_pods.resources | length > 0
    #   tags: ['icinga']

    # - name: Initialize Icinga Web 2 admin user directly
    #   kubernetes.core.k8s_exec:
    #     namespace: "{{ ns_monitoring }}"
    #     pod: "{{ icinga_pod }}"
    #     container: "icinga2"
    #     command:
    #       - /bin/bash
    #       - -c
    #       - |
    #         set -e
    #         mkdir -p /etc/icingaweb2
    #         echo "[users]" > /etc/icingaweb2/authentication.ini
    #         echo 'backend = "ini"' >> /etc/icingaweb2/authentication.ini
    #         echo "[admin]" > /etc/icingaweb2/users.ini
    #         echo 'password = admin' >> /etc/icingaweb2/users.ini
    #         echo "[Administrators]" > /etc/icingaweb2/roles.ini
    #         echo 'users = "admin"' >> /etc/icingaweb2/roles.ini
    #         echo 'permissions = "*"' >> /etc/icingaweb2/roles.ini
    #         chown -R www-data:www-data /etc/icingaweb2 2>/dev/null || true
    #         echo "[OK] Plaintext Icinga admin user created (admin/admin)"
    #   when: icinga_pod is defined
    #   tags: ['icinga']

    # - name: Restart Icinga2 Deployment
    #   kubernetes.core.k8s:
    #     state: patched
    #     kind: Deployment
    #     name: icinga2
    #     namespace: "{{ ns_monitoring }}"
    #     definition:
    #       spec:
    #         template:
    #           metadata:
    #             annotations:
    #               redeploy-timestamp: "{{ lookup('pipe', 'date +%s') }}"
    #   tags: ['icinga']

    # -------------------- 6Ô∏è‚É£ PrometheusRule --------------------
    - name: Apply PrometheusRule for Scenario 1
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('file', 'files/rules/prometheusrule-scenario1.yaml') | from_yaml }}"
      tags: ['rules', 'scenario1']

    - name: Apply PrometheusRule for Scenario 2 (Chaos)
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('file', 'files/rules/prometheusrule-scenario2.yaml') | from_yaml }}"
      tags: ['rules', 'scenario2']

    - name: Apply PrometheusRule for Scenario 3 (Failure Recovery)
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('file', 'files/rules/prometheusrule-scenario3.yaml') | from_yaml }}"
      tags: ['rules', 'scenario3']

    - name: Pause briefly for rule propagation
      ansible.builtin.pause:
        seconds: 5
      tags: ['rules']

    # -------------------- 7Ô∏è‚É£ Grafana ConfigMaps --------------------
    - name: Apply Grafana datasource provider
      kubernetes.core.k8s:
        state: present
        src: files/grafana/provisioning/dashboards-configmap.yaml
      tags: ['grafana']

    - name: Apply Grafana dashboards (Scenario 1, 2, 3)
      kubernetes.core.k8s:
        state: present
        src: manifests/grafana-dashboards-cm.yaml
      tags: ['grafana']

    - name: Re-apply Grafana datasources (ensure Loki visible)
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('file', 'files/grafana/provisioning/datasources-configmap.yaml') | from_yaml }}"
      tags: ['grafana']

    - name: Restart Grafana Deployment to reload dashboards and datasources
      kubernetes.core.k8s:
        state: patched
        kind: Deployment
        name: kps-grafana
        namespace: "{{ ns_monitoring }}"
        definition:
          spec:
            template:
              metadata:
                annotations:
                  redeploy-timestamp: "{{ lookup('pipe', 'date +%s') }}"
      tags: ['grafana']


    # # -------------------- 8Ô∏è‚É£ Cluster Autoscaler (Helm + metrics) --------------------
    # - name: Install / Upgrade Cluster Autoscaler (Helm)
    #   kubernetes.core.helm:
    #     name: cluster-autoscaler
    #     chart_ref: autoscaler/cluster-autoscaler
    #     release_namespace: kube-system
    #     create_namespace: false
    #     wait: true
    #     timeout: 600s
    #     chart_version: "{{ ca_chart_version }}"
    #     values: "{{ lookup('file', 'files/values-cluster-autoscaler.yaml') | from_yaml }}"
    #   tags: ['ca']

    # - name: Expose Cluster Autoscaler metrics (Service)
    #   kubernetes.core.k8s:
    #     state: present
    #     definition: "{{ lookup('file', 'manifests/service-cluster-autoscaler.yaml') | from_yaml }}"
    #   tags: ['ca']

    # - name: Create ServiceMonitor for Cluster Autoscaler
    #   kubernetes.core.k8s:
    #     state: present
    #     definition: "{{ lookup('file', 'manifests/servicemonitor-cluster-autoscaler.yaml') | from_yaml }}"
    #   tags: ['ca']

    # # -------------------- 9Ô∏è‚É£ HPA cho ·ª©ng d·ª•ng PetClinic (Namespace: app_ns) --------------------
    # - name: Deploy HPA for PetClinic (api-gateway)
    #   kubernetes.core.k8s:
    #     state: present
    #     src: manifests/hpa-petclinic.yaml
    #   when: deploy_hpa_examples | default(true)
    #   tags: ['hpa']

    # # -------------------------------------------------------------
    # # üö¶ Deploy Locust load testing components (Scenario 1 + 2)
    # # -------------------------------------------------------------
    # - name: Deploy Locust Benchmark Tools (Scenario 1 + 2)
    #   import_role:
    #     name: locust
    #   vars:
    #     locust_base_path: "files/locust-scripts"
    #     locust_scripts:
    #       - "locust-hpa.py"
    #       - "locust-ca.py"
    #   tags: ['locust']
